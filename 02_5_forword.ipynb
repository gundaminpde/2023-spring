{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOblc++7TSOxDqjcLJahAKJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gundaminpde/2023-spring/blob/main/02_5_forword.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhDwvG7j3Zq6",
        "outputId": "23602f8b-6e66-46de-8739-35196efa8155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST DataSets 준비 완료\n",
            "loss = _______  time = ________  n = ______ | [TrainAcc] [TestAcc]\n",
            "loss =  2.7990  time =  30.5042  n = 000001 |  0.1417     0.1461\n",
            "loss =  2.1151  time =  59.6543  n = 000002 |  0.1262     0.1258\n",
            "loss =  2.0780  time =  89.8155  n = 000003 |  0.1827     0.1862\n",
            "loss =  1.9628  time = 120.3245  n = 000004 |  0.1939     0.1999\n",
            "loss =  1.9029  time = 151.0767  n = 000005 |  0.2326     0.2403\n",
            "loss =  1.6285  time = 182.0165  n = 000006 |  0.2487     0.2541\n",
            "loss =  1.9789  time = 212.5545  n = 000007 |  0.2675     0.2708\n",
            "loss =  1.3084  time = 243.1781  n = 000008 |  0.2996     0.3019\n",
            "loss =  1.1188  time = 273.6971  n = 000009 |  0.3140     0.3179\n",
            "loss =  1.6079  time = 303.8991  n = 000010 |  0.3209     0.3238\n"
          ]
        }
      ],
      "source": [
        "\n",
        " \n",
        "\n",
        "##### 파이썬으로 구현한 Forward Propagation Code\n",
        "\n",
        "#### 함부로 실행하면 후회함.\n",
        "\n",
        "############# 출처:인공지능 100점을 위한 파이썬 수학 – 임성국, BJPUBLiC\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "#############################\n",
        "\n",
        "# MNIST 데이터 입력\n",
        "#############################\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "(x_train, t_train), (x_test, t_test) = mnist.load_data()\n",
        "t_trainlbl, t_testlbl = t_train, t_test\n",
        "\n",
        "# 28X28 을 784 로 수정\n",
        "x_train = x_train.reshape(60000,784)    # 주석 (1)\n",
        "x_test = x_test.reshape(10000,784)   \n",
        "\n",
        "# one-hot label\n",
        "T0 = np.zeros((t_train.size, 10))    #(60000,10) = 000\n",
        "T1 = np.zeros((t_test.size, 10))    #(10000,10) = 000\n",
        "\n",
        "for idx in range(t_train.size): T0[idx][t_train[idx]] = 1    #(3))\n",
        "for idx in range(t_test.size): T1[idx][t_test[idx]] = 1\n",
        "\n",
        "t_train, t_test = T0, T1\n",
        "\n",
        "# normalize 0.0 ~ 1.0\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "print('MNIST DataSets 준비 완료')\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "#################################\n",
        "\n",
        "#  함수정의\n",
        "\n",
        "###################################\n",
        "\n",
        "# 미분함수\n",
        "def numerical_diff(f, x):\n",
        "    h = 1e-4    # 0.0001\n",
        "    nd_coef = np.zeros_like(x)\n",
        "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        index = it.multi_index\n",
        "        tmp = x[index]\n",
        "        x[index] = tmp + h\n",
        "        fxh2 = f()    # f(x+h)\n",
        "        x[index] = tmp - h\n",
        "        fxh1 = f()    # f(x-h)\n",
        "        nd_coef[index] = (fxh2 - fxh1) / (2*h)\n",
        "        x[index] = tmp\n",
        "        it.iternext()\n",
        "    return nd_coef\n",
        "\n",
        "# 시그모이드\n",
        "def sigmoid(x):\n",
        "    return 1 / (1+np.exp(-x))\n",
        "\n",
        "# 소프트맥스\n",
        "def softmax(x):\n",
        "    if x.ndim == 1:  # 기본 1개 처리과정 , 벡터입력\n",
        "        x = x - np.max(x)\n",
        "        return np.exp(x) / np.sum(np.exp(x))\n",
        "    if x.ndim == 2:  # 배치용 n 개 처리, 행렬입력\n",
        "        x = x.T - np.max(x.T, axis=0)\n",
        "        return (np.exp(x) / np.sum(np.exp(x), axis=0)).T\n",
        "\n",
        "# 크로스엔트로피오차\n",
        "def cee(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)  # 크기가 1xN 인 2차원 행렬로 재구성\n",
        "        y = y.reshape(1, y.size)\n",
        "    result = -np.sum(t * np.log(y + 1e-7))  / y.shape[0]\n",
        "    return result \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "############################ \n",
        "\n",
        "##   프로세스별클래스생성\n",
        "\n",
        "###########################\n",
        "\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        result = x.copy()\n",
        "        result[self.mask] = 0\n",
        "        return result\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.out = sigmoid(x)\n",
        "        return self.out\n",
        "\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W    # W0, W1\n",
        "        self.b = b    # b0, b1\n",
        "        self.x = None\n",
        "        self.dW = None    # W0, W1 의 기울기\n",
        "        self.db = None    # b0, b1 의 기울기\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        result = np.dot(self.x, self.W) + self.b\n",
        "        return result\n",
        "\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.y = None    # 출력(계산결과)\n",
        "        self.t = None    # 정답(MNIST레이블)\n",
        "       \n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        result = cee(self.y, self.t)\n",
        "        return result\n",
        "\n",
        " \n",
        "\n",
        "###########################\n",
        "\n",
        "# 네트워크클래스생성\n",
        "\n",
        "#######################3\n",
        "\n",
        "class SimpleNetwork:\n",
        "    def __init__(self, inputx, hidden, outy, weight):\n",
        "        # 가중치 초기화\n",
        "        self.netMat = {}\n",
        "        self.netMat['W0'] = weight * np.random.randn(inputx, hidden)\n",
        "        self.netMat['b0'] = np.zeros(hidden)\n",
        "        self.netMat['W1'] = weight * np.random.randn(hidden, outy)\n",
        "        self.netMat['b1'] = np.zeros(outy)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.netLayers = {}\n",
        "        self.netLayers['Affine1'] = Affine(self.netMat['W0'],\n",
        "                                           self.netMat['b0'])\n",
        "        self.netLayers['Relu1'] = Relu()\n",
        "        self.netLayers['Affine2'] = Affine(self.netMat['W1'],\n",
        "                                           self.netMat['b1'])\n",
        "        self.netLayers['Softmax'] = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        x = self.netLayers['Affine1'].forward(x)\n",
        "        x = self.netLayers['Relu1'].forward(x)\n",
        "        x = self.netLayers['Affine2'].forward(x)\n",
        "        return x\n",
        "       \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.netLayers['Softmax'].forward(y, t)\n",
        "   \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "       \n",
        "    def numerical_gradient(self, x, t):\n",
        "        lossfunc = lambda : self.loss(x, t)\n",
        "        grads = {}\n",
        "        grads['W0'] = numerical_diff(lossfunc, self.netMat['W0'])\n",
        "        grads['b0'] = numerical_diff(lossfunc, self.netMat['b0'])\n",
        "        grads['W1'] = numerical_diff(lossfunc, self.netMat['W1'])\n",
        "        grads['b1'] = numerical_diff(lossfunc, self.netMat['b1'])\n",
        "        return grads\n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        " \n",
        "\n",
        "##################################\n",
        "\n",
        "# 미분을이용한학습과검증\n",
        "\n",
        "#################################\n",
        "\n",
        "import time    # Using Time Module(시간측정)\n",
        "t1 = time.time()    # save nowTime(현재 시간 측정)\n",
        "\n",
        "train_size = x_train.shape[0]    # size of TrainData (입력데이터 크기) 60000\n",
        "lr = 0.1    # learning rate(학습률)\n",
        "iter = 0    # Iternation Number (반복횟수)\n",
        "\n",
        "iters_num = 10\n",
        "batch_size = 20\n",
        "iter_per_epoch = 1\n",
        "\n",
        "network = SimpleNetwork(inputx=784, hidden=50, outy=10, weight = 0.2)\n",
        "\n",
        "print('loss = _______  time = ________  n = ______ | [TrainAcc] [TestAcc]')\n",
        "\n",
        "for i in range(iters_num):    # 10\n",
        "    batch_mask = np.random.choice(train_size, batch_size)    #(1)주석\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "   \n",
        "    # 기울기 계산\n",
        "    grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분(편 미분) 방식\n",
        "\n",
        "    # 갱신\n",
        "    for key in ('W0', 'b0', 'W1', 'b1'): network.netMat[key] -= lr * grad[key]\n",
        "   \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    iter = iter + 1\n",
        "\n",
        "    print('loss = {:7.4f}  '.format(loss), end='')\n",
        "    print('time = {:8.4f}  '.format(time.time()-t1), end='')   \n",
        "    print('n = {:06d} |  {:6.4f}  {:9.4f}'.format(iter, train_acc, test_acc))\n",
        "\n",
        " "
      ]
    }
  ]
}